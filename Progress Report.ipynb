{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from fancyimpute import KNN\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from zenrows import ZenRowsClient\n",
    "client = ZenRowsClient(\"fa1a58c4dda65f20ad3ea8423dbe1b7ea3b0ced7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_stats(webpage, stat):\n",
    "    \"\"\"\n",
    "    Given a player's college stats page, returns the total stats for the player.\n",
    "    \"\"\"\n",
    "    stat_html = webpage.select(f'td[data-stat=\"{stat}\"]')\n",
    "    if stat_html:\n",
    "        return stat_html[-1].get_text()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all years in the dataset.\n",
    "current = False\n",
    "START_YEAR = 2012\n",
    "END_YEAR = 2023\n",
    "\n",
    "STATS_LIST = [\n",
    "    #Defense and Fumbles\n",
    "    'tackles_solo',\n",
    "    'tackles_assists',\n",
    "    'tackles_total', \n",
    "    'tackles_loss',\n",
    "    'sacks', \n",
    "    'def_int',\n",
    "    'def_int_yds', \n",
    "    'def_int_td', \n",
    "    'pass_defended',\n",
    "    'fumbles_rec', \n",
    "    'fumbles_rec_yds',\n",
    "    'fumbles_rec_td',\n",
    "    'fumbles_forced', \n",
    "\n",
    "    # Passing\n",
    "    'pass_cmp', \n",
    "    'pass_att',\n",
    "    'pass_cmp_pct',\n",
    "    'pass_yds',\n",
    "    'pass_td',\n",
    "    'pass_int',\n",
    "    'pass_rating',\n",
    "\n",
    "    # Receiving & Rushing \n",
    "    'rec',\n",
    "    'rec_yds',\n",
    "    'rec_yds_per_rec',\n",
    "    'rec_td',\n",
    "    'rush_att',\n",
    "    'rush_yds',\n",
    "    'rush_yds_per_att',\n",
    "    'rush_td',\n",
    "    'scrim_att',\n",
    "    'scrim_yds',\n",
    "    'scrim_yds_per_att',\n",
    "    'scrim_td',\n",
    "\n",
    "    # Punt & Kick Returns \n",
    "    'punt_ret',\n",
    "    'punt_ret_yds',\n",
    "    'punt_ret_yds_per_ret',\n",
    "    'punt_ret_td',\n",
    "    'kick_ret',\n",
    "    'kick_ret_yds',\n",
    "    'kick_ret_yds_per_ret',\n",
    "    'kick_ret_td'\n",
    "    \n",
    "    # Punting & Kicking\n",
    "    'xpm',\n",
    "    'xpa',\n",
    "    'xp_pct',\n",
    "    'fgm',\n",
    "    'fga',\n",
    "    'fg_pct',\n",
    "    'kick_points',\n",
    "    'punt',\n",
    "    'punt_yds',\n",
    "    'punt_yds_per_punt'\n",
    "    ]\n",
    "\n",
    "\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    df = pd.DataFrame()\n",
    "    print(year)\n",
    "    # Read draft data\n",
    "    url = f\"https://www.pro-football-reference.com/draft/{year}-combine.htm\"\n",
    "    response = requests.get(url)\n",
    "    webpage = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Get the names of the players\n",
    "    names_html = webpage.select(\"tbody .left:nth-child(1)\")\n",
    "    all_names = [name.get_text() for name in names_html]\n",
    "    names = [name for name in all_names if name != \"Player\"]\n",
    "    num_players = len(names)\n",
    "\n",
    "    # Get the position of the players\n",
    "    pos_html = webpage.select(\"th+ td\")\n",
    "    pos = [pos.get_text() for pos in pos_html]\n",
    "    pick = [0] * num_players\n",
    "    round_ = [0] * num_players\n",
    "\n",
    "    # Get draft data if this is not the current year.\n",
    "    if not current:\n",
    "        draft_html = webpage.select(\".right+ .left\")\n",
    "        draft_info = [info.get_text() for info in draft_html]\n",
    "        draft_info = [\"Undrafted / 0th / 0th / 0\" if info == \"\" else info for info in draft_info]\n",
    "        draft_spots = [info.split(\" / \") for info in draft_info]\n",
    "        round_ = [int(spot[1][0]) for spot in draft_spots]\n",
    "        pick = [int(''.join(filter(str.isdigit, spot[2]))) for spot in draft_spots]\n",
    "\n",
    "    #Get school data\n",
    "    college_elements = webpage.select('td.left + .left')\n",
    "    college = [element.get_text() for element in college_elements]\n",
    "\n",
    "    df[\"Name\"] = names\n",
    "    df[\"Position\"] = pos\n",
    "    df[\"College\"] = college\n",
    "    df[\"Round\"] = round_\n",
    "    df[\"Pick\"] = pick\n",
    "    \n",
    "    # Get the links to the player's college stats\n",
    "    stat_urls = []\n",
    "    for link in webpage.select('td[data-stat=\"college\"]'):\n",
    "        if link.find('a'):\n",
    "            stat_urls.append(link.find('a').get('href'))\n",
    "        else:\n",
    "            stat_urls.append(None)\n",
    "\n",
    "    df[\"Stat URL\"] = stat_urls\n",
    "\n",
    "    # Get height\n",
    "    height_html = webpage.select(\"td[data-stat='height']\")\n",
    "    height = [h.get_text() for h in height_html]\n",
    "    height = [h.split(\"-\") for h in height]\n",
    "    new_height = []\n",
    "    for h in height:\n",
    "        if len(h) == 2:\n",
    "            new_height.append((int(h[0]) * 12 + int(h[1])))\n",
    "        else:\n",
    "            new_height.append(math.nan)\n",
    "    df[\"Height\"] = new_height\n",
    "\n",
    "    # Get weight\n",
    "    weight_html = webpage.select(\"td[data-stat='weight']\")\n",
    "    weight = [w.get_text() for w in weight_html]\n",
    "    weight = [int(w) if w != \"\" else math.nan for w in weight]\n",
    "    df[\"Weight\"] = weight\n",
    "\n",
    "    # Get 40 yard dash\n",
    "    forty_html = webpage.select(\"td[data-stat='forty_yd']\")\n",
    "    forty = [f.get_text() for f in forty_html]\n",
    "    forty = [float(f) if f != \"\" else math.nan for f in forty]  \n",
    "    df[\"40 Yard Dash\"] = forty\n",
    "\n",
    "    # Get bench press\n",
    "    bench_html = webpage.select(\"td[data-stat='bench_reps']\")\n",
    "    bench = [b.get_text() for b in bench_html]\n",
    "    bench = [int(b) if b != \"\" else math.nan for b in bench]\n",
    "    df[\"Bench Press\"] = bench\n",
    "\n",
    "    # Get vertical jump\n",
    "    vertical_html = webpage.select(\"td[data-stat='vertical']\")\n",
    "    vertical = [v.get_text() for v in vertical_html]\n",
    "    vertical = [float(v) if v != \"\" else math.nan for v in vertical]\n",
    "    df[\"Vertical Jump\"] = vertical\n",
    "\n",
    "    # Get broad jump\n",
    "    broad_html = webpage.select(\"td[data-stat='broad_jump']\")\n",
    "    broad = [b.get_text() for b in broad_html]\n",
    "    broad = [int(b) if b != \"\" else math.nan for b in broad]\n",
    "    df[\"Broad Jump\"] = broad\n",
    "\n",
    "    # Get 3 cone drill\n",
    "    cone_html = webpage.select(\"td[data-stat='cone']\")\n",
    "    cone = [c.get_text() for c in cone_html]\n",
    "    cone = [float(c) if c != \"\" else math.nan for c in cone]\n",
    "    df[\"3 Cone Drill\"] = cone\n",
    "\n",
    "    # Get shuttle\n",
    "    shuttle_html = webpage.select(\"td[data-stat='shuttle']\")\n",
    "    shuttle = [s.get_text() for s in shuttle_html]\n",
    "    shuttle = [float(s) if s != \"\" else math.nan for s in shuttle]\n",
    "    df[\"Shuttle\"] = shuttle\n",
    "\n",
    "    df.dropna(subset=[\"Stat URL\"], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    urls  = df[\"Stat URL\"]\n",
    "    all_stats = {}\n",
    "    \n",
    "    for url in tqdm(urls):\n",
    "        stats = {}\n",
    "        response = client.get(url)\n",
    "        webpage = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Get conference from stat page\n",
    "        conf_html = webpage.select('td[data-stat=\"conf_abbr\"]')\n",
    "        if conf_html:\n",
    "            conf = conf_html[0].get_text()\n",
    "            stats['conf_abbr'] = conf\n",
    "        else:\n",
    "            stats['conf_abbr'] = None\n",
    "\n",
    "        # Get games played and seasons played\n",
    "        games_html = webpage.select('td[data-stat=\"g\"]')\n",
    "        \n",
    "        if games_html:\n",
    "            season = 0\n",
    "            games_played = 0\n",
    "            for game in games_html:\n",
    "                if game.get_text() != \"\":\n",
    "                    games_played += int(game.get_text())\n",
    "                    season += 1\n",
    "            \n",
    "            stats['games'] = games_played\n",
    "            stats['seasons'] = season\n",
    "        else:\n",
    "            stats['games'] = None\n",
    "            stats['seasons'] = None\n",
    "\n",
    "        # Get total stats\n",
    "        for stat in STATS_LIST:\n",
    "            stats[stat] = get_total_stats(webpage, stat)\n",
    "\n",
    "        all_stats[url] = stats\n",
    "\n",
    "    stat_df = pd.DataFrame(all_stats).T\n",
    "    stat_df.index.name = \"Stat URL\"\n",
    "    new_df = pd.merge(df, stat_df, on=\"Stat URL\")\n",
    "    new_df[\"Year\"] = year\n",
    "    new_df.to_csv(f\"data/{year}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine all years data into one csv file called \"combined_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all csv files\n",
    "csv_files = ['2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
    "\n",
    "# Create an empty list to store the dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop over the list of csv files\n",
    "for csv in csv_files:\n",
    "    # Read each csv file into a DataFrame and append it to the list\n",
    "    dfs.append(pd.read_csv('data/' + csv + '.csv'))\n",
    "\n",
    "# Concatenate all dataframes in the list into one dataframe\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df.to_csv('data/combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identifying and Dropping Columns with Less Than 10% Data Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store original column names\n",
    "original_columns = df.columns\n",
    "\n",
    "# Drop columns with less than 10% data available\n",
    "df = df.dropna(thresh=(0.1 * len(df)), axis=1)\n",
    "\n",
    "# Get the remaining column names after dropping\n",
    "remaining_columns = df.columns\n",
    "\n",
    "# Find the dropped column names\n",
    "dropped_columns = original_columns.difference(remaining_columns)\n",
    "\n",
    "# Print the dropped column names\n",
    "print(dropped_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing Missing Values Using K-Nearest Neighbors (KNN) Algorithm and Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting important columns from the original DataFrame\n",
    "imp_df = df[['Position', 'Height', 'Weight', '40 Yard Dash', 'Bench Press', \n",
    "             'Vertical Jump', 'Broad Jump', '3 Cone Drill', 'Shuttle', \n",
    "             'tackles_solo', 'tackles_assists', 'tackles_loss', 'sacks', \n",
    "             'def_int', 'def_int_yds', 'def_int_td', 'pass_defended',\n",
    "             'fumbles_rec', 'fumbles_rec_yds', 'fumbles_rec_td', 'fumbles_forced', \n",
    "             'rec', 'rec_yds', 'rec_yds_per_rec', 'rec_td', 'rush_att', 'rush_yds', \n",
    "             'rush_yds_per_att', 'rush_td', 'scrim_att', 'scrim_yds', \n",
    "             'scrim_yds_per_att', 'scrim_td']]\n",
    "\n",
    "# Initialize a label encoder for encoding categorical 'Position' column\n",
    "label_encoder = LabelEncoder()\n",
    "imp_df.loc[:, 'Position'] = label_encoder.fit_transform(imp_df['Position'])\n",
    "\n",
    "# Impute missing values using KNN algorithm with k=5\n",
    "imp_df = KNN(k=5).fit_transform(imp_df)\n",
    "imp_df = pd.DataFrame(imp_df)\n",
    "\n",
    "# Rename columns of the DataFrame\n",
    "imp_df.columns = ['Position', 'Height', 'Weight', '40 Yard Dash', 'Bench Press', \n",
    "             'Vertical Jump', 'Broad Jump', '3 Cone Drill', 'Shuttle', \n",
    "             'tackles_solo', 'tackles_assists', 'tackles_loss', 'sacks', \n",
    "             'def_int', 'def_int_yds', 'def_int_td', 'pass_defended',\n",
    "             'fumbles_rec', 'fumbles_rec_yds', 'fumbles_rec_td', 'fumbles_forced', \n",
    "             'rec', 'rec_yds', 'rec_yds_per_rec', 'rec_td', 'rush_att', 'rush_yds', \n",
    "             'rush_yds_per_att', 'rush_td', 'scrim_att', 'scrim_yds', \n",
    "             'scrim_yds_per_att', 'scrim_td']\n",
    "\n",
    "# Round the values in the DataFrame to 2 decimal places\n",
    "imp_df = imp_df.round(2)\n",
    "\n",
    "# Replace the selected columns in the original DataFrame with the imputed values\n",
    "df[['Height', 'Weight', '40 Yard Dash', 'Bench Press', 'Vertical Jump', 'Broad Jump',\n",
    "    '3 Cone Drill', 'Shuttle', 'tackles_solo', 'tackles_assists', 'tackles_loss', 'sacks', \n",
    "    'def_int', 'def_int_yds', 'def_int_td', 'pass_defended', 'fumbles_rec', 'fumbles_rec_yds',\n",
    "    'fumbles_rec_td', 'fumbles_forced', 'rec', 'rec_yds', 'rec_yds_per_rec', 'rec_td', 'rush_att',\n",
    "     'rush_yds', 'rush_yds_per_att', 'rush_td', 'scrim_att', 'scrim_yds','scrim_yds_per_att', 'scrim_td']] = imp_df.drop('Position', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing Missing Games and Seasons Values Using KNN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns 'Position', 'games', and 'seasons' from the original DataFrame\n",
    "imp_df = df[[\"Position\", \"games\", \"seasons\"]]\n",
    "\n",
    "# Encode the 'Position' column using a label encoder\n",
    "imp_df.loc[:, 'Position'] = label_encoder.fit_transform(imp_df[\"Position\"])\n",
    "\n",
    "# Impute missing values for 'games' and 'seasons' columns using KNN algorithm with k=10\n",
    "imp_df=fancyimpute.KNN(k=10).fit_transform(imp_df)\n",
    "imp_df = pd.DataFrame(imp_df)\n",
    "\n",
    "# Round the values in the DataFrame to the nearest integer\n",
    "imp_df = imp_df.round(0)\n",
    "\n",
    "# Replace the missing values in the original DataFrame for 'Games' and 'Seasons' with the imputed values\n",
    "df[[\"Games\", \"Seasons\"]] = imp_df.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating Total Tackles and Exporting Imputed Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tackles_total'] = df['tackles_solo'] + df['tackles_assists']\n",
    "df['tackles_total'] = df['tackles_total'].round(0)\n",
    "\n",
    "# Export the imputed data to a CSV file\n",
    "df.to_csv('data/imputed_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
